{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Questions"
      ],
      "metadata": {
        "id": "AHMus7mqZdAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:  What is Ensemble Learning in machine learning? Explain the key idea behind it.**\n",
        "  - It is a technique where multiple models are trained and combined to solve the same problem.\n",
        "  - The key idea behind it is that a group of models working together can get better performance than any individual model on its own.\n",
        "  - Pridictions on different models are combined to get better pridictions."
      ],
      "metadata": {
        "id": "0mnS9T5nZhGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between Bagging and Boosting?**\n",
        "  - Bagging:-\n",
        "    - It creates different training datasets by random sampling with replacement.\n",
        "    - All models are trained independently and parallely.\n",
        "    - It reduce variance of the model.\n",
        "  - Boosting:-\n",
        "    - In this technique each new model is trained by focusing more on the misclassified samples from previous models.\n",
        "    - All models are trained one after another.\n",
        "    - It reduce bias and variance."
      ],
      "metadata": {
        "id": "NMa5zBCpahF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**\n",
        "  - It is a statistical resampling technique used for to create multiple new datasets from the original training data.\n",
        "  - It randomly select N samples with replacement from the dataset.\n",
        "  - Some samples will appear multiple times and some will not.\n",
        "  - Then each bootstrap sample is used to train a different model."
      ],
      "metadata": {
        "id": "FDAdB_fPbfiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**\n",
        "  - Out-of-Bag are the samples refer to the subset of the training data not included in a given bootstrap sample.\n",
        "  - It evaluates ensemble models like Random Forest without needing an external validation or test set."
      ],
      "metadata": {
        "id": "TED2XGE9cN88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest. **\n",
        "  -"
      ],
      "metadata": {
        "id": "B1tKxN4idWCV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4fWPtY8Zb89",
        "outputId": "6f1aa252-aa0f-4602-ceb3-cb0ec17b8eaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Feature  Importance\n",
            "20          worst radius    0.154410\n",
            "23            worst area    0.128149\n",
            "27  worst concave points    0.118846\n",
            "7    mean concave points    0.097132\n",
            "22       worst perimeter    0.078753\n"
          ]
        }
      ],
      "source": [
        "#Question 6: Write a Python program to:\n",
        "#● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "#● Train a Random Forest Classifier\n",
        "#● Print the top 5 most important features based on feature importance scores.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=2)\n",
        "model.fit(X, y)\n",
        "\n",
        "importances = pd.DataFrame({'Feature': X.columns, 'Importance': model.feature_importances_})\n",
        "print(importances.sort_values(by='Importance', ascending=False).head(5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "#● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "#● Evaluate its accuracy and compare with a single Decision Tree\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
        "\n",
        "tree = DecisionTreeClassifier(random_state=2)\n",
        "tree.fit(X_train, y_train)\n",
        "tree_pred = tree.predict(X_test)\n",
        "tree_acc = accuracy_score(y_test, tree_pred)\n",
        "\n",
        "bag_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=2\n",
        ")\n",
        "bag_model.fit(X_train, y_train)\n",
        "bag_pred = bag_model.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {tree_acc:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {bag_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGSaNKEqePF1",
        "outputId": "575a76ab-8a1a-4739-b6f4-21175b2e6256"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.9556\n",
            "Bagging Classifier Accuracy: 0.9778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#● Train a Random Forest Classifier\n",
        "#● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "#● Print the best parameters and final accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=2)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10, 15]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Final Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVoWUdYaefW5",
        "outputId": "e6f55a3c-aa41-438d-883a-f8b0504f5a62"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy: 0.9778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "#● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "#● Compare their Mean Squared Errors (MSE)\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
        "\n",
        "bag_model = BaggingRegressor(random_state=2)\n",
        "bag_model.fit(X_train, y_train)\n",
        "bag_pred = bag_model.predict(X_test)\n",
        "bag_mse = mean_squared_error(y_test, bag_pred)\n",
        "\n",
        "rf_model = RandomForestRegressor(random_state=2)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(f\"Bagging Regressor MSE: {bag_mse:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTMGUfv1fEje",
        "outputId": "b38f7afd-b5ce-4f94-f8bb-df422a353925"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2979\n",
            "Random Forest Regressor MSE: 0.2635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance.**\n",
        "  - Choose between Bagging or Boosting:-\n",
        "    - Bagging reduces variance by averaging predictions from multiple independent models.\n",
        "    - Suitable if the dataset is large and diverse.\n",
        "    - Boosting reduces bias and often improves accuracy by sequentially correcting errors from previous models."
      ],
      "metadata": {
        "id": "-lVftAeDfd0X"
      }
    }
  ]
}