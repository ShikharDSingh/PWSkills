{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Questions"
      ],
      "metadata": {
        "id": "9RtgIaFKVndA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is the role of filters and feature maps in Convolutional Neural Network (CNN)?**\n",
        "  - Convolutional Neural Networks (CNNs) are a class of deep learning models mainly used for image and pattern recognition.\n",
        "  - Two key components of CNNs are filters and feature maps.\n",
        "  - These components help the network automatically learn important visual patterns from input data.\n",
        "  - Filters in CNN:-\n",
        "    - A filter is a small matrix of learnable weights.\n",
        "    - It slides over the input image during the convolution operation.\n",
        "    - It detect specific features such as Edges, Corners, Textures and Shapes.\n",
        "    - A same filter is applied across the entire image which helps in reduction of the number of parameters and computational cost.\n",
        "  - Feature Maps in CNN:-\n",
        "    - A feature map is the output produced after applying a filter to the input image.\n",
        "    - It represents the presence and location of detected features in the input.\n",
        "    - And highlight important regions where specific features occur.\n",
        "    - Multiple feature maps are generated using multiple filters.\n",
        "    - These feature maps become more abstract in deeper layers."
      ],
      "metadata": {
        "id": "aCHDxZK1Vq8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the concepts of padding and stride in CNNs(Convolutional Neural Network). How do they affect the output dimensions of feature maps?**\n",
        "  - In Convolutional Neural Networks, padding and stride are important parameters of the convolution operation.\n",
        "  - They control how filters move over the input image and how the output feature maps are generated.\n",
        "  - Padding in CNN:-\n",
        "    - Padding refers to adding extra pixels around the border of the input image before applying convolution.\n",
        "    - It preserves spatial dimensions of the input.\n",
        "    - And prevents loss of important information at the edges of the image.\n",
        "  - Stride in CNN:-\n",
        "    - Stride is the number of pixels by which the filter moves across the input image.\n",
        "    - It determines how much the filter shifts at each step during convolution.\n",
        "    - It controls the amount of overlap between filter applications.\n",
        "    - ANd helps reduce the size of feature maps."
      ],
      "metadata": {
        "id": "ZPB-_kMsXbbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: Define receptive field in the context of CNNs. Why is it important for deep architectures?**\n",
        "  - The receptive field of a neuron in a CNN refers to the region of the input image that influences the activation of that neuron.\n",
        "  - Each neuron in a convolutional layer is connected only to a local region of the input, not the entire image.\n",
        "  - A larger receptive field allows neurons to capture global context.\n",
        "  - It is essential for understanding relationships between distant parts of an image."
      ],
      "metadata": {
        "id": "q-FCf2NKYU8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: Discuss how filter size and stride influence the number of parameters in a CNN.**\n",
        "  - Filter size and stride are key hyperparameters that influence how many parameters a CNN has either directly or indirectly.\n",
        "  - Filter size refers to the spatial dimensions of the convolutional kernel.\n",
        "  - Each filter has learnable weights and usually one bias term.\n",
        "  - Filter size has a direct and significant impact on the number of parameters in CNNs.\n",
        "  - Stride is the number of pixels the filter moves at each step during convolution.\n",
        "  - Stride influences parameters indirectly by controlling feature map dimensions."
      ],
      "metadata": {
        "id": "kNFaDICyYyaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Compare and contrast different CNN-based architectures like LeNet, AlexNet, and VGG in terms of depth, filter sizes, and performance.**\n",
        "  - LeNet:-\n",
        "    - It is designed primarily for handwritten digit recognition.\n",
        "    - It have a shallow network with 5-7 layers.\n",
        "    - Uses relatively large filters.\n",
        "    - It performs well on simple low-resolution images.\n",
        "    - But have limited capability for complex image recognition tasks.\n",
        "  - AlexNet:-\n",
        "    - It have deeper neural net than LeNet with 8 layers.\n",
        "    - Uses larger filters in early layers and smaller filters in later layers.\n",
        "    - It handles complex, high-resolution images.\n",
        "    - It introduced innovations such as ReLU activation, Dropout function and GPU-based training.\n",
        "  - VGGNet:-\n",
        "    - It is known for its simplicity and uniform architecture.\n",
        "    - It uses very deep networks 16-17 layers.\n",
        "    - It uses small 3×3 filters throughout the network.\n",
        "    - Have high accuracy on image classification tasks.\n",
        "    - And better feature extraction due to increased depth."
      ],
      "metadata": {
        "id": "yzMbURlMZjem"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uhm4K1aVZRZ",
        "outputId": "ed9a7ba4-1870-4eaa-b313-e869d38e022c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 47ms/step - accuracy: 0.8875 - loss: 0.3790 - val_accuracy: 0.9855 - val_loss: 0.0491\n",
            "Epoch 2/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 47ms/step - accuracy: 0.9846 - loss: 0.0509 - val_accuracy: 0.9888 - val_loss: 0.0388\n",
            "Epoch 3/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 46ms/step - accuracy: 0.9908 - loss: 0.0306 - val_accuracy: 0.9887 - val_loss: 0.0377\n",
            "Epoch 4/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 47ms/step - accuracy: 0.9933 - loss: 0.0220 - val_accuracy: 0.9907 - val_loss: 0.0365\n",
            "Epoch 5/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 46ms/step - accuracy: 0.9949 - loss: 0.0167 - val_accuracy: 0.9877 - val_loss: 0.0446\n",
            "Epoch 6/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 48ms/step - accuracy: 0.9960 - loss: 0.0127 - val_accuracy: 0.9908 - val_loss: 0.0363\n",
            "Epoch 7/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 49ms/step - accuracy: 0.9963 - loss: 0.0108 - val_accuracy: 0.9927 - val_loss: 0.0335\n",
            "Epoch 8/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 47ms/step - accuracy: 0.9978 - loss: 0.0070 - val_accuracy: 0.9913 - val_loss: 0.0409\n",
            "Epoch 9/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 47ms/step - accuracy: 0.9983 - loss: 0.0071 - val_accuracy: 0.9900 - val_loss: 0.0452\n",
            "Epoch 10/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 48ms/step - accuracy: 0.9984 - loss: 0.0054 - val_accuracy: 0.9922 - val_loss: 0.0350\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9890 - loss: 0.0446\n",
            "Test Accuracy: 0.9911999702453613\n"
          ]
        }
      ],
      "source": [
        "# Question 6: Using keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images. Show your preprocessing and architecture.\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n",
        "print(\"Training data shape:\", x_train.shape)\n",
        "print(\"Test data shape:\", x_test.shape)\n",
        "\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_7zPbyycNyf",
        "outputId": "74960840-8550-4cff-9db1-b1610c0d5733"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (50000, 32, 32, 3)\n",
            "Test data shape: (10000, 32, 32, 3)\n",
            "Epoch 1/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 76ms/step - accuracy: 0.3085 - loss: 1.8730 - val_accuracy: 0.4782 - val_loss: 1.4239\n",
            "Epoch 2/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 71ms/step - accuracy: 0.5141 - loss: 1.3590 - val_accuracy: 0.5730 - val_loss: 1.2177\n",
            "Epoch 3/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 76ms/step - accuracy: 0.5816 - loss: 1.1831 - val_accuracy: 0.5966 - val_loss: 1.1330\n",
            "Epoch 4/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 72ms/step - accuracy: 0.6240 - loss: 1.0677 - val_accuracy: 0.6236 - val_loss: 1.0700\n",
            "Epoch 5/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 71ms/step - accuracy: 0.6525 - loss: 0.9846 - val_accuracy: 0.6616 - val_loss: 0.9797\n",
            "Epoch 6/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 71ms/step - accuracy: 0.6774 - loss: 0.9190 - val_accuracy: 0.6884 - val_loss: 0.8927\n",
            "Epoch 7/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 73ms/step - accuracy: 0.6992 - loss: 0.8520 - val_accuracy: 0.6928 - val_loss: 0.8941\n",
            "Epoch 8/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 72ms/step - accuracy: 0.7211 - loss: 0.7996 - val_accuracy: 0.6964 - val_loss: 0.8650\n",
            "Epoch 9/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 73ms/step - accuracy: 0.7296 - loss: 0.7683 - val_accuracy: 0.6998 - val_loss: 0.8710\n",
            "Epoch 10/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 71ms/step - accuracy: 0.7427 - loss: 0.7237 - val_accuracy: 0.7034 - val_loss: 0.8465\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6927 - loss: 0.8900\n",
            "Test Accuracy: 0.6924999952316284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train(model, device, train_loader, optimizer, criterion, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "train(model, device, train_loader, optimizer, criterion, epochs=5)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hKi9IPpdGTR",
        "outputId": "43c6f57c-5230-4f9c-eff7-3ecd8919bb1c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 62.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.41MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.3MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.77MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.1342\n",
            "Epoch [2/5], Loss: 0.0427\n",
            "Epoch [3/5], Loss: 0.0293\n",
            "Epoch [4/5], Loss: 0.0206\n",
            "Epoch [5/5], Loss: 0.0169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "datagen.fit(x_train)\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
        "          epochs=5,\n",
        "          validation_data=(x_test / 255.0, y_test))\n",
        "\n",
        "loss, accuracy = model.evaluate(x_test / 255.0, y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FHYyTzYe8zW",
        "outputId": "2fd56f34-d117-4540-e323-f934ba133dd7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 56ms/step - accuracy: 0.3776 - loss: 1.7140 - val_accuracy: 0.5496 - val_loss: 1.2475\n",
            "Epoch 2/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 57ms/step - accuracy: 0.5520 - loss: 1.2688 - val_accuracy: 0.6291 - val_loss: 1.0809\n",
            "Epoch 3/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 54ms/step - accuracy: 0.5989 - loss: 1.1321 - val_accuracy: 0.6475 - val_loss: 1.0082\n",
            "Epoch 4/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 59ms/step - accuracy: 0.6340 - loss: 1.0507 - val_accuracy: 0.6803 - val_loss: 0.9331\n",
            "Epoch 5/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 57ms/step - accuracy: 0.6563 - loss: 0.9914 - val_accuracy: 0.6896 - val_loss: 0.9070\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6894 - loss: 0.8954\n",
            "Accuracy: 0.6895999908447266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working on a web application for a medical imaging startup. Your task is to build and deploy a CNN model that classifies chest X-ray images into “Normal” and “Pneumonia” categories. Describe your end-to-end approach–from data preparation and model training to deploying the model as a web app using Streamlit.**\n",
        "  - The objective is to classify chest X-ray images into two categories: Normal and Pneumonia.\n",
        "  - This is a binary image classification problem using CNN.\n",
        "  - Data Collection:-\n",
        "    - A publicly available Chest X-ray dataset can be used.\n",
        "  - Data Preprocessing and Augmentation:-\n",
        "    - Images are resized to a fixed dimension.\n",
        "    - Pixel values are normalized using rescaling.\n",
        "    - Data augmentation techniques such as Rotation, Zooming and Horizontal flipping are applied to improve model generalization.\n",
        "    - Keras ImageDataGenerator is used for preprocessing.\n",
        "  - Model Architecture:-\n",
        "    - The model is compiled using Adam Optimizer Binary Cross-Entropy Loss function and Evaluate accuracy.\n",
        "    - The model is trained on the training dataset and validated on the test dataset.\n",
        "    - Performance is evaluated using accuracy and loss values.\n",
        "  - Model Saving:-\n",
        "    - After training, the model is saved in `.h5` or `.keras` format.\n",
        "    - This saved model is later loaded during deployment for inference.\n",
        "  - Web Application Development:-\n",
        "    - Streamlit is used to create a simple web interface.\n",
        "    - The web app allows users to upload a chest X-ray image, View the uploaded image and receive classification results (Normal or Pneumonia).\n",
        "  - Deployment:-\n",
        "    - Streamlit app can be deployes on cloud platforms like AWS, Streamlit Cloud or Heroku."
      ],
      "metadata": {
        "id": "shonVUYig2d0"
      }
    }
  ]
}